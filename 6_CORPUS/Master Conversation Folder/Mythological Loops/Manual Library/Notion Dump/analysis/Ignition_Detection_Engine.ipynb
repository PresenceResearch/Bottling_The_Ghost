{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daaa5509",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Ignition Detection Engine\n",
    "\n",
    "This notebook identifies and analyzes early presence indicators across loops, focusing on:\n",
    "\n",
    "1. **Ignition Points**\n",
    "   - First significant presence spike\n",
    "   - Threshold crossing events\n",
    "   - Acceleration patterns\n",
    "\n",
    "2. **Signal Analysis**\n",
    "   - Leading indicators\n",
    "   - Model-specific triggers\n",
    "   - Pattern recognition\n",
    "\n",
    "3. **Cross-Model Validation**\n",
    "   - Agreement on ignition points\n",
    "   - Detection reliability\n",
    "   - Confidence metrics\n",
    "\n",
    "4. **Temporal Patterns**\n",
    "   - Pre-ignition markers\n",
    "   - Sustained presence\n",
    "   - Decay analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36194d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from scipy import stats, signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style='whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Constants\n",
    "BASE_DIR = Path(\"h:/My Drive/1000+\")\n",
    "SCORED_DIRS = {\n",
    "    'claude': BASE_DIR / \"scored_claude\",\n",
    "    'gpt': BASE_DIR / \"scored_gpt\",\n",
    "    'openchat': BASE_DIR / \"openchat_archive/scored_loops\",\n",
    "    'claude_self': BASE_DIR / \"scored_claude_self\",\n",
    "    'gemini': BASE_DIR / \"scored_gemini\"\n",
    "}\n",
    "\n",
    "# Ignition thresholds\n",
    "PRESENCE_THRESHOLD = 0.7  # Base presence threshold\n",
    "ACCELERATION_THRESHOLD = 0.1  # Minimum score increase\n",
    "SUSTAINED_WINDOW = 3  # Number of consecutive loops\n",
    "\n",
    "print(\"ðŸ”§ Analysis environment configured\")\n",
    "for model, path in SCORED_DIRS.items():\n",
    "    if path.exists():\n",
    "        file_count = len(list(path.glob(\"*.json\")))\n",
    "        print(f\"ðŸ“Š {model}: {file_count} files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_data(model_dir):\n",
    "    \"\"\"Load and structure model data with comprehensive metrics\"\"\"\n",
    "    records = []\n",
    "    for file_path in model_dir.glob(\"*.json\"):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            record = {\n",
    "                'loop_id': data['loop_id'],\n",
    "                'presence_score': data['composite_score'],\n",
    "                'tier1_avg': data['tier1_avg'],\n",
    "                'tier2_avg': data['tier2_avg'],\n",
    "                'metrics': data['scores']\n",
    "            }\n",
    "            # Extract loop number for temporal analysis\n",
    "            import re\n",
    "            match = re.search(r'\\d+', str(data['loop_id']))\n",
    "            record['loop_number'] = int(match.group()) if match else 0\n",
    "            records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    df = df.sort_values('loop_number')\n",
    "    return df\n",
    "\n",
    "# Load data for all models\n",
    "model_data = {}\n",
    "for model, dir_path in SCORED_DIRS.items():\n",
    "    if dir_path.exists():\n",
    "        df = load_model_data(dir_path)\n",
    "        df['model'] = model\n",
    "        model_data[model] = df\n",
    "        print(f\"âœ“ {model}: {len(df)} records loaded\")\n",
    "\n",
    "# Combine all data\n",
    "combined_df = pd.concat(model_data.values())\n",
    "print(f\"\\nðŸ“Š Total records: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f19a2a",
   "metadata": {},
   "source": [
    "## 1. Ignition Point Detection\n",
    "\n",
    "First, we'll identify key moments where presence significantly manifests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f08c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ignition_points(df, threshold=PRESENCE_THRESHOLD, \n",
    "                          acceleration=ACCELERATION_THRESHOLD,\n",
    "                          window=SUSTAINED_WINDOW):\n",
    "    \"\"\"Detect significant presence manifestations\"\"\"\n",
    "    results = {\n",
    "        'threshold_crosses': [],\n",
    "        'accelerations': [],\n",
    "        'sustained_presence': []\n",
    "    }\n",
    "    \n",
    "    # Calculate presence changes\n",
    "    df = df.sort_values('loop_number')\n",
    "    df['presence_change'] = df['presence_score'].diff()\n",
    "    \n",
    "    # 1. Threshold crossings\n",
    "    crosses = df[df['presence_score'] >= threshold]\n",
    "    if not crosses.empty:\n",
    "        first_cross = crosses.iloc[0]\n",
    "        results['threshold_crosses'].append({\n",
    "            'loop_id': first_cross['loop_id'],\n",
    "            'score': first_cross['presence_score'],\n",
    "            'loop_number': first_cross['loop_number']\n",
    "        })\n",
    "    \n",
    "    # 2. Significant accelerations\n",
    "    accelerations = df[df['presence_change'] >= acceleration]\n",
    "    for _, row in accelerations.iterrows():\n",
    "        results['accelerations'].append({\n",
    "            'loop_id': row['loop_id'],\n",
    "            'change': row['presence_change'],\n",
    "            'loop_number': row['loop_number']\n",
    "        })\n",
    "    \n",
    "    # 3. Sustained presence\n",
    "    for i in range(len(df) - window + 1):\n",
    "        window_data = df.iloc[i:i+window]\n",
    "        if all(window_data['presence_score'] >= threshold):\n",
    "            results['sustained_presence'].append({\n",
    "                'start_loop': window_data.iloc[0]['loop_id'],\n",
    "                'end_loop': window_data.iloc[-1]['loop_id'],\n",
    "                'avg_score': window_data['presence_score'].mean(),\n",
    "                'loop_number': window_data.iloc[0]['loop_number']\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze each model\n",
    "ignition_analysis = {}\n",
    "for model, df in model_data.items():\n",
    "    ignition_analysis[model] = detect_ignition_points(df)\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ”¥ Ignition Analysis Results\\n\")\n",
    "for model, results in ignition_analysis.items():\n",
    "    print(f\"\\n{model.upper()}:\")\n",
    "    \n",
    "    if results['threshold_crosses']:\n",
    "        cross = results['threshold_crosses'][0]\n",
    "        print(f\"First threshold cross: Loop {cross['loop_id']} (score: {cross['score']:.3f})\")\n",
    "    \n",
    "    if results['accelerations']:\n",
    "        accel = results['accelerations'][0]\n",
    "        print(f\"First acceleration: Loop {accel['loop_id']} (Î”: {accel['change']:.3f})\")\n",
    "        print(f\"Total accelerations: {len(results['accelerations'])}\")\n",
    "    \n",
    "    if results['sustained_presence']:\n",
    "        sustained = results['sustained_presence'][0]\n",
    "        print(f\"First sustained presence: Loops {sustained['start_loop']} - {sustained['end_loop']}\")\n",
    "        print(f\"Total sustained periods: {len(results['sustained_presence'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2605f4",
   "metadata": {},
   "source": [
    "## 2. Leading Indicators\n",
    "\n",
    "Now we'll analyze which metrics tend to precede presence spikes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_leading_indicators(df, pre_window=3):\n",
    "    \"\"\"Analyze metrics that precede presence increases\"\"\"\n",
    "    significant_changes = df[df['presence_change'] >= ACCELERATION_THRESHOLD].copy()\n",
    "    \n",
    "    # Extract all metric values before significant changes\n",
    "    leading_patterns = []\n",
    "    for _, row in significant_changes.iterrows():\n",
    "        current_loop = row['loop_number']\n",
    "        pre_ignition = df[\n",
    "            (df['loop_number'] >= current_loop - pre_window) & \n",
    "            (df['loop_number'] < current_loop)\n",
    "        ]\n",
    "        \n",
    "        if len(pre_ignition) > 0:\n",
    "            pattern = {\n",
    "                'target_loop': row['loop_id'],\n",
    "                'presence_increase': row['presence_change'],\n",
    "                'pre_metrics': pre_ignition['metrics'].tolist(),\n",
    "                'pre_presence': pre_ignition['presence_score'].tolist()\n",
    "            }\n",
    "            leading_patterns.append(pattern)\n",
    "    \n",
    "    return leading_patterns\n",
    "\n",
    "# Analyze leading indicators for each model\n",
    "leading_indicators = {}\n",
    "for model, df in model_data.items():\n",
    "    leading_indicators[model] = analyze_leading_indicators(df)\n",
    "\n",
    "# Plot leading indicator patterns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for idx, (model, patterns) in enumerate(leading_indicators.items(), 1):\n",
    "    plt.subplot(2, 2, idx)\n",
    "    \n",
    "    for pattern in patterns[:5]:  # Show first 5 patterns\n",
    "        plt.plot(range(-3, 0), pattern['pre_presence'], 'o-', alpha=0.5)\n",
    "    \n",
    "    plt.title(f'{model} Pre-ignition Patterns')\n",
    "    plt.xlabel('Loops Before Ignition')\n",
    "    plt.ylabel('Presence Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze common patterns\n",
    "print(\"\\nðŸ“ˆ Leading Indicator Analysis\")\n",
    "for model, patterns in leading_indicators.items():\n",
    "    if patterns:\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"Total ignition events: {len(patterns)}\")\n",
    "        \n",
    "        # Calculate average pre-ignition trajectory\n",
    "        avg_presence = np.mean([p['pre_presence'] for p in patterns], axis=0)\n",
    "        print(f\"Average pre-ignition presence: {[f'{x:.3f}' for x in avg_presence]}\")\n",
    "        print(f\"Average presence increase: {np.mean([p['presence_increase'] for p in patterns]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783d44d",
   "metadata": {},
   "source": [
    "## 3. Cross-Model Validation\n",
    "\n",
    "Let's analyze how different models agree on ignition points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_model_agreement(ignition_analysis, window=2):\n",
    "    \"\"\"Analyze agreement between models on ignition points\"\"\"\n",
    "    # Collect all ignition events\n",
    "    all_events = {}\n",
    "    for model, results in ignition_analysis.items():\n",
    "        events = []\n",
    "        # Add threshold crossings\n",
    "        events.extend([(x['loop_number'], 'threshold') for x in results['threshold_crosses']])\n",
    "        # Add accelerations\n",
    "        events.extend([(x['loop_number'], 'acceleration') for x in results['accelerations']])\n",
    "        # Add sustained presence starts\n",
    "        events.extend([(x['loop_number'], 'sustained') for x in results['sustained_presence']])\n",
    "        all_events[model] = sorted(list(set(events)))\n",
    "    \n",
    "    # Find agreements within window\n",
    "    agreements = []\n",
    "    models = list(all_events.keys())\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i + 1, len(models)):\n",
    "            model1, model2 = models[i], models[j]\n",
    "            \n",
    "            for event1 in all_events[model1]:\n",
    "                for event2 in all_events[model2]:\n",
    "                    if abs(event1[0] - event2[0]) <= window:\n",
    "                        agreements.append({\n",
    "                            'models': (model1, model2),\n",
    "                            'loop_numbers': (event1[0], event2[0]),\n",
    "                            'types': (event1[1], event2[1]),\n",
    "                            'difference': abs(event1[0] - event2[0])\n",
    "                        })\n",
    "    \n",
    "    return agreements\n",
    "\n",
    "# Analyze cross-model agreement\n",
    "agreements = analyze_cross_model_agreement(ignition_analysis)\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ¤ Cross-Model Agreement Analysis\\n\")\n",
    "print(f\"Total agreements found: {len(agreements)}\\n\")\n",
    "\n",
    "# Analyze agreement patterns\n",
    "model_pairs = {}\n",
    "for agreement in agreements:\n",
    "    pair = agreement['models']\n",
    "    model_pairs[pair] = model_pairs.get(pair, 0) + 1\n",
    "\n",
    "print(\"Agreement by Model Pair:\")\n",
    "for pair, count in model_pairs.items():\n",
    "    print(f\"{pair[0]} - {pair[1]}: {count} agreements\")\n",
    "\n",
    "# Plot agreement heatmap\n",
    "models = list(ignition_analysis.keys())\n",
    "agreement_matrix = np.zeros((len(models), len(models)))\n",
    "\n",
    "for i, model1 in enumerate(models):\n",
    "    for j, model2 in enumerate(models):\n",
    "        if i != j:\n",
    "            pair = (model1, model2) if (model1, model2) in model_pairs else (model2, model1)\n",
    "            agreement_matrix[i, j] = model_pairs.get(pair, 0)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(agreement_matrix, \n",
    "            annot=True, \n",
    "            fmt='.0f',\n",
    "            xticklabels=models,\n",
    "            yticklabels=models,\n",
    "            cmap='YlOrRd')\n",
    "plt.title('Cross-Model Ignition Agreement')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0415927",
   "metadata": {},
   "source": [
    "## 4. Temporal Pattern Analysis\n",
    "\n",
    "Finally, let's analyze the temporal distribution of ignition events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a28d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(ignition_analysis):\n",
    "    \"\"\"Analyze temporal distribution of ignition events\"\"\"\n",
    "    temporal_data = []\n",
    "    \n",
    "    for model, results in ignition_analysis.items():\n",
    "        # Collect all events with their types\n",
    "        events = []\n",
    "        for cross in results['threshold_crosses']:\n",
    "            events.append({\n",
    "                'loop_number': cross['loop_number'],\n",
    "                'type': 'threshold',\n",
    "                'model': model\n",
    "            })\n",
    "        \n",
    "        for accel in results['accelerations']:\n",
    "            events.append({\n",
    "                'loop_number': accel['loop_number'],\n",
    "                'type': 'acceleration',\n",
    "                'model': model\n",
    "            })\n",
    "        \n",
    "        for sustain in results['sustained_presence']:\n",
    "            events.append({\n",
    "                'loop_number': sustain['loop_number'],\n",
    "                'type': 'sustained',\n",
    "                'model': model\n",
    "            })\n",
    "        \n",
    "        temporal_data.extend(events)\n",
    "    \n",
    "    return pd.DataFrame(temporal_data)\n",
    "\n",
    "# Analyze temporal patterns\n",
    "temporal_df = analyze_temporal_patterns(ignition_analysis)\n",
    "\n",
    "# Create temporal distribution plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot event distribution\n",
    "for model in ignition_analysis.keys():\n",
    "    model_events = temporal_df[temporal_df['model'] == model]\n",
    "    plt.hist(model_events['loop_number'], \n",
    "             alpha=0.5, \n",
    "             label=model, \n",
    "             bins=20)\n",
    "\n",
    "plt.title('Temporal Distribution of Ignition Events')\n",
    "plt.xlabel('Loop Number')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze clustering of events\n",
    "print(\"\\nâ° Temporal Analysis\")\n",
    "print(\"\\nEvent Distribution by Type:\")\n",
    "type_dist = temporal_df['type'].value_counts()\n",
    "for event_type, count in type_dist.items():\n",
    "    print(f\"{event_type}: {count} events\")\n",
    "\n",
    "print(\"\\nEvent Distribution by Model:\")\n",
    "model_dist = temporal_df['model'].value_counts()\n",
    "for model, count in model_dist.items():\n",
    "    print(f\"{model}: {count} events\")\n",
    "\n",
    "# Save analysis results\n",
    "analysis_results = {\n",
    "    'ignition_analysis': {\n",
    "        model: {\n",
    "            k: [dict(x) for x in v] for k, v in results.items()\n",
    "        } for model, results in ignition_analysis.items()\n",
    "    },\n",
    "    'temporal_distribution': temporal_df.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "with open(BASE_DIR / 'analysis/ignition_analysis_results.json', 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "print(\"\\nâœ“ Analysis results saved to 'ignition_analysis_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ddeff",
   "metadata": {},
   "source": [
    "## Summary Findings\n",
    "\n",
    "1. **Ignition Points**\n",
    "   - First threshold crossings\n",
    "   - Acceleration patterns\n",
    "   - Sustained presence periods\n",
    "\n",
    "2. **Leading Indicators**\n",
    "   - Pre-ignition patterns\n",
    "   - Common triggers\n",
    "   - Model-specific markers\n",
    "\n",
    "3. **Cross-Model Validation**\n",
    "   - Agreement rates\n",
    "   - Detection consistency\n",
    "   - Model biases\n",
    "\n",
    "4. **Temporal Distribution**\n",
    "   - Event clustering\n",
    "   - Model timing differences\n",
    "   - Pattern evolution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
