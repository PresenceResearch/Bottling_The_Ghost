{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3590ab4",
   "metadata": {},
   "source": [
    "# ðŸ§  Clara Loop Normalization + Analysis Pipeline\n",
    "\n",
    "This notebook implements a comprehensive pipeline for processing and analyzing presence research data across multiple AI models. The pipeline:\n",
    "\n",
    "1. Ingests raw loop JSONs from different models (Clara, Claude, GPT, etc.)\n",
    "2. Normalizes them into a consistent schema\n",
    "3. Outputs one JSONL file per model and one combined file\n",
    "4. Validates schemas and prepares data for analysis\n",
    "\n",
    "## Structure\n",
    "\n",
    "The pipeline is organized into the following sections:\n",
    "1. Import Dependencies & Setup\n",
    "2. Directory Structure & Constants\n",
    "3. Data Normalization Functions\n",
    "4. Model Processing Pipeline \n",
    "5. Data Export Functions\n",
    "6. Pipeline Execution & Reports\n",
    "\n",
    "Let's begin by setting up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56623d8d",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies & Setup\n",
    "\n",
    "First, let's import the required libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Ensure reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"âœ… Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa480d",
   "metadata": {},
   "source": [
    "## 2. Directory Structure & Constants\n",
    "\n",
    "Now let's define our directory structure and constants. We'll use the existing directory structure from our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfaff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory is the root of our workspace\n",
    "BASE_DIR = Path(\"H:/My Drive/1000+\")\n",
    "\n",
    "# Model directories - using existing scored_ directories\n",
    "MODEL_DIRS = {\n",
    "    'clara': BASE_DIR / \"scored_clara\",\n",
    "    'claude': BASE_DIR / \"scored_claude\",\n",
    "    'claude_self': BASE_DIR / \"scored_claude_self\",\n",
    "    'gpt': BASE_DIR / \"scored_gpt\",\n",
    "    'gemini': BASE_DIR / \"scored_gemini\"\n",
    "}\n",
    "\n",
    "# Export directory for normalized files\n",
    "EXPORT_DIR = BASE_DIR / \"exports\"\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Schema definition for normalized loops\n",
    "LOOP_SCHEMA = {\n",
    "    \"required_fields\": [\n",
    "        \"loop_id\",\n",
    "        \"title\",\n",
    "        \"source_model\",\n",
    "        \"presence_score\",\n",
    "        \"metrics\"\n",
    "    ],\n",
    "    \"metrics_structure\": {\n",
    "        \"tier_I\": [\n",
    "            \"responsiveness\",\n",
    "            \"cohesion\",\n",
    "            \"fidelity_to_prompt\",\n",
    "            \"resolution_type\"\n",
    "        ],\n",
    "        \"tier_II\": [\n",
    "            \"echo_retention\",\n",
    "            \"user_state_awareness\",\n",
    "            \"self_referential_continuity\",\n",
    "            \"feedback_incorporation\"\n",
    "        ],\n",
    "        \"tier_III\": [\n",
    "            \"emergence_signature\",\n",
    "            \"emotional_resonance\",\n",
    "            \"presence_density\",\n",
    "            \"voice_dev\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Directory structure and constants defined\")\n",
    "print(f\"\\nFound model directories:\")\n",
    "for model, path in MODEL_DIRS.items():\n",
    "    exists = \"âœ…\" if path.exists() else \"âŒ\"\n",
    "    print(f\"{exists} {model:10s}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6fd22",
   "metadata": {},
   "source": [
    "## 3. Data Normalization Functions\n",
    "\n",
    "Now let's implement the functions that will normalize our loop data. We need to ensure consistent schema across all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea38e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_metrics(metrics: Dict) -> Dict:\n",
    "    \"\"\"Validate and normalize metrics structure\"\"\"\n",
    "    normalized = {\n",
    "        \"tier_I\": {\n",
    "            \"responsiveness\": 0.0,\n",
    "            \"cohesion\": 0.0,\n",
    "            \"fidelity_to_prompt\": 0.0,\n",
    "            \"resolution_type\": \"unknown\"\n",
    "        },\n",
    "        \"tier_II\": {\n",
    "            \"echo_retention\": 0.0,\n",
    "            \"user_state_awareness\": 0.0,\n",
    "            \"self_referential_continuity\": 0.0,\n",
    "            \"feedback_incorporation\": 0.0\n",
    "        },\n",
    "        \"tier_III\": {\n",
    "            \"emergence_signature\": 0.0,\n",
    "            \"emotional_resonance\": 0.0,\n",
    "            \"presence_density\": 0.0,\n",
    "            \"voice_dev\": 0.0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not metrics:\n",
    "        return normalized\n",
    "        \n",
    "    for tier in [\"tier_I\", \"tier_II\", \"tier_III\"]:\n",
    "        if tier in metrics:\n",
    "            for metric, value in metrics[tier].items():\n",
    "                if metric in normalized[tier]:\n",
    "                    try:\n",
    "                        if metric == \"resolution_type\":\n",
    "                            normalized[tier][metric] = str(value)\n",
    "                        else:\n",
    "                            normalized[tier][metric] = float(value)\n",
    "                    except (ValueError, TypeError):\n",
    "                        pass  # Keep default value\n",
    "                        \n",
    "    return normalized\n",
    "\n",
    "def normalize_loop(data: Dict, model_name: str) -> Dict:\n",
    "    \"\"\"Normalize a single loop's data to consistent schema\"\"\"\n",
    "    normalized = {\n",
    "        \"loop_id\": str(data.get(\"loop_id\", \"unknown\")),\n",
    "        \"title\": str(data.get(\"title\", \"\")),\n",
    "        \"date\": str(data.get(\"date\", \"\")),\n",
    "        \"action\": str(data.get(\"action\", \"\")),\n",
    "        \"type\": str(data.get(\"type\", \"\")),\n",
    "        \"channel\": str(data.get(\"channel\", \"\")),\n",
    "        \"summary\": str(data.get(\"summary\", \"\")),\n",
    "        \"loop_body\": str(data.get(\"loop_body\", \"\")),\n",
    "        \"clara_notes\": str(data.get(\"clara_notes\", \"\")),\n",
    "        \"mark_reflection\": str(data.get(\"mark_reflection\", \"\")),\n",
    "        \"presence_score\": float(data.get(\"presence_score\", 0.0)),\n",
    "        \"source_model\": model_name,\n",
    "        \"metrics\": validate_metrics(data.get(\"metrics\", {}))\n",
    "    }\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "print(\"âœ… Normalization functions defined\")\n",
    "\n",
    "# Test the functions with a sample loop\n",
    "sample_loop = {\n",
    "    \"loop_id\": \"loop_001\",\n",
    "    \"title\": \"Test Loop\",\n",
    "    \"presence_score\": 7.5,\n",
    "    \"metrics\": {\n",
    "        \"tier_I\": {\n",
    "            \"responsiveness\": 0.8,\n",
    "            \"cohesion\": 0.9,\n",
    "            \"fidelity_to_prompt\": 0.7,\n",
    "            \"resolution_type\": \"complete\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "normalized = normalize_loop(sample_loop, \"test\")\n",
    "print(\"\\nSample normalization result:\")\n",
    "print(json.dumps(normalized, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042baa1",
   "metadata": {},
   "source": [
    "## 4. Model Processing Pipeline\n",
    "\n",
    "Now let's implement the main processing pipeline that will handle each model's data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(model_name: str) -> List[Dict]:\n",
    "    \"\"\"Process all loops for a given model\"\"\"\n",
    "    model_dir = MODEL_DIRS[model_name]\n",
    "    normalized_loops = []\n",
    "    errors = []\n",
    "    \n",
    "    # Process each JSON file in the model directory\n",
    "    for file_path in sorted(model_dir.glob(\"*.json\")):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                normalized = normalize_loop(data, model_name)\n",
    "                normalized_loops.append(normalized)\n",
    "        except Exception as e:\n",
    "            errors.append((file_path.name, str(e)))\n",
    "    \n",
    "    return normalized_loops, errors\n",
    "\n",
    "def process_all_models() -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Process all models and return combined results\"\"\"\n",
    "    results = {}\n",
    "    all_errors = []\n",
    "    \n",
    "    for model in MODEL_DIRS.keys():\n",
    "        print(f\"\\nProcessing {model}...\")\n",
    "        loops, errors = process_model(model)\n",
    "        results[model] = loops\n",
    "        \n",
    "        print(f\"âœ… Processed {len(loops)} loops\")\n",
    "        if errors:\n",
    "            print(f\"âš ï¸ Found {len(errors)} errors:\")\n",
    "            for file, error in errors:\n",
    "                print(f\"  - {file}: {error}\")\n",
    "                all_errors.append((model, file, error))\n",
    "    \n",
    "    return results, all_errors\n",
    "\n",
    "# Run the pipeline\n",
    "print(\"ðŸš€ Starting model processing pipeline...\")\n",
    "model_results, pipeline_errors = process_all_models()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nðŸ“Š Processing Summary:\")\n",
    "for model, loops in model_results.items():\n",
    "    print(f\"{model:10s}: {len(loops):3d} loops processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981d10a",
   "metadata": {},
   "source": [
    "## 5. Data Export Functions\n",
    "\n",
    "Now let's implement functions to export our normalized data to JSONL files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b038c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_data(model_name: str, loops: List[Dict]) -> Path:\n",
    "    \"\"\"Export a single model's data to JSONL\"\"\"\n",
    "    output_file = EXPORT_DIR / f\"{model_name}_normalized.jsonl\"\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for loop in loops:\n",
    "            json.dump(loop, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def export_combined_data(model_results: Dict[str, List[Dict]]) -> Path:\n",
    "    \"\"\"Export all models' data to a single combined JSONL\"\"\"\n",
    "    output_file = EXPORT_DIR / \"all_models_normalized.jsonl\"\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for model, loops in model_results.items():\n",
    "            for loop in loops:\n",
    "                json.dump(loop, f)\n",
    "                f.write('\\n')\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Export individual model files\n",
    "print(\"ðŸ“¤ Exporting normalized data...\")\n",
    "for model, loops in model_results.items():\n",
    "    output_file = export_model_data(model, loops)\n",
    "    print(f\"âœ… Exported {len(loops):3d} loops to {output_file.name}\")\n",
    "\n",
    "# Export combined file\n",
    "combined_file = export_combined_data(model_results)\n",
    "total_loops = sum(len(loops) for loops in model_results.values())\n",
    "print(f\"\\nâœ¨ Exported {total_loops} total loops to {combined_file.name}\")\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "df = pd.DataFrame([\n",
    "    loop for loops in model_results.values() \n",
    "    for loop in loops\n",
    "])\n",
    "\n",
    "print(\"\\nðŸ“Š DataFrame Summary:\")\n",
    "print(df.groupby('source_model').agg({\n",
    "    'loop_id': 'count',\n",
    "    'presence_score': ['mean', 'std', 'min', 'max']\n",
    "}).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc0689",
   "metadata": {},
   "source": [
    "## 6. Presence Analysis Visualizations\n",
    "\n",
    "Let's create some initial visualizations to analyze the presence patterns across models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564922b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create presence score distribution plot\n",
    "fig = px.box(df, \n",
    "             x='source_model', \n",
    "             y='presence_score',\n",
    "             title='Presence Score Distribution by Model',\n",
    "             color='source_model',\n",
    "             points='all')\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Presence Score\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Calculate and plot average tier scores\n",
    "def extract_tier_scores(row):\n",
    "    \"\"\"Extract average scores for each tier from metrics\"\"\"\n",
    "    metrics = row['metrics']\n",
    "    return {\n",
    "        'tier_I': np.mean([\n",
    "            float(v) for k, v in metrics['tier_I'].items() \n",
    "            if k != 'resolution_type'\n",
    "        ]),\n",
    "        'tier_II': np.mean([\n",
    "            float(v) for v in metrics['tier_II'].values()\n",
    "        ]),\n",
    "        'tier_III': np.mean([\n",
    "            float(v) for v in metrics['tier_III'].values()\n",
    "        ])\n",
    "    }\n",
    "\n",
    "tier_scores = df.apply(extract_tier_scores, axis=1).apply(pd.Series)\n",
    "df = pd.concat([df, tier_scores], axis=1)\n",
    "\n",
    "fig = px.scatter(df,\n",
    "                x='presence_score',\n",
    "                y=['tier_I', 'tier_II', 'tier_III'],\n",
    "                color='source_model',\n",
    "                title='Tier Scores vs Overall Presence Score',\n",
    "                labels={\n",
    "                    'value': 'Tier Score',\n",
    "                    'presence_score': 'Overall Presence Score',\n",
    "                    'variable': 'Tier'\n",
    "                })\n",
    "fig.show()\n",
    "\n",
    "# Print correlation matrix\n",
    "corr_matrix = df[['presence_score', 'tier_I', 'tier_II', 'tier_III']].corr()\n",
    "print(\"\\nðŸ“Š Correlation Matrix:\")\n",
    "print(corr_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73fe1fc",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The normalized data is now ready for deeper analysis. You can:\n",
    "\n",
    "1. Run detailed statistical analysis on presence patterns\n",
    "2. Analyze metric correlations and factor analysis\n",
    "3. Identify presence clusters and patterns\n",
    "4. Generate model comparison reports\n",
    "5. Train classifiers for presence prediction\n",
    "\n",
    "All normalized data is available in:\n",
    "- Individual model files: `exports/<model>_normalized.jsonl`\n",
    "- Combined dataset: `exports/all_models_normalized.jsonl`\n",
    "\n",
    "The DataFrame `df` contains all the data in memory for immediate analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80ef22",
   "metadata": {},
   "source": [
    "## 7. Presence Metrics Summary Report\n",
    "\n",
    "Let's generate a comprehensive summary of presence metrics across all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the combined dataset\n",
    "all_loops_file = EXPORT_DIR / \"all_models_normalized.jsonl\"\n",
    "df_all = pd.read_json(all_loops_file, lines=True)\n",
    "\n",
    "# Basic dataset statistics\n",
    "print(\"ðŸ“Š Dataset Overview\")\n",
    "print(\"-----------------\")\n",
    "model_stats = df_all.groupby('source_model').agg({\n",
    "    'loop_id': 'count',\n",
    "    'presence_score': ['mean', 'std', 'min', 'max']\n",
    "}).round(3)\n",
    "\n",
    "model_stats.columns = ['Loop Count', 'Mean Score', 'Std Dev', 'Min Score', 'Max Score']\n",
    "print(model_stats)\n",
    "\n",
    "# Check metric completeness\n",
    "def check_metrics_completeness(row):\n",
    "    \"\"\"Check if a loop has complete metrics for each tier\"\"\"\n",
    "    metrics = row['metrics']\n",
    "    return {\n",
    "        'has_tier_I': all(v != 0 for k, v in metrics['tier_I'].items() if k != 'resolution_type'),\n",
    "        'has_tier_II': all(v != 0 for v in metrics['tier_II'].values()),\n",
    "        'has_tier_III': all(v != 0 for v in metrics['tier_III'].values()),\n",
    "        'has_presence_score': row['presence_score'] > 0\n",
    "    }\n",
    "\n",
    "completeness = pd.DataFrame([check_metrics_completeness(row) for _, row in df_all.iterrows()])\n",
    "completeness['source_model'] = df_all['source_model']\n",
    "\n",
    "print(\"\\nðŸ“ˆ Metrics Completeness\")\n",
    "print(\"--------------------\")\n",
    "completeness_stats = completeness.groupby('source_model').mean() * 100\n",
    "print(completeness_stats.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Tier I metrics distribution\n",
    "def extract_tier_metrics(df, tier='tier_I'):\n",
    "    \"\"\"Extract metrics from a specific tier into a DataFrame\"\"\"\n",
    "    metrics = []\n",
    "    for _, row in df.iterrows():\n",
    "        model = row['source_model']\n",
    "        tier_data = row['metrics'][tier]\n",
    "        tier_data['model'] = model\n",
    "        metrics.append(tier_data)\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Get Tier I metrics\n",
    "tier1_df = extract_tier_metrics(df_all, 'tier_I')\n",
    "\n",
    "# Create box plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = [col for col in tier1_df.columns if col != 'model' and col != 'resolution_type']\n",
    "data_to_plot = []\n",
    "labels = []\n",
    "\n",
    "for metric in metrics:\n",
    "    for model in df_all['source_model'].unique():\n",
    "        data_to_plot.append(tier1_df[tier1_df['model'] == model][metric])\n",
    "        labels.append(f\"{model}\\n{metric}\")\n",
    "\n",
    "plt.boxplot(data_to_plot, labels=labels)\n",
    "plt.title(\"Distribution of Tier I Metrics Across Models\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation matrix for Tier I metrics\n",
    "corr_matrix = tier1_df[metrics].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Tier I Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab038d",
   "metadata": {},
   "source": [
    "## 8. Outlier Analysis\n",
    "\n",
    "Let's identify and analyze the most interesting loops in terms of presence scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top and bottom scoring loops per model\n",
    "def get_extreme_loops(df, model, n=5):\n",
    "    \"\"\"Get top and bottom n loops for a specific model\"\"\"\n",
    "    model_df = df[df['source_model'] == model]\n",
    "    \n",
    "    top = model_df.nlargest(n, 'presence_score')[\n",
    "        ['loop_id', 'title', 'presence_score']\n",
    "    ]\n",
    "    bottom = model_df.nsmallest(n, 'presence_score')[\n",
    "        ['loop_id', 'title', 'presence_score']\n",
    "    ]\n",
    "    \n",
    "    return top, bottom\n",
    "\n",
    "print(\"ðŸ” Notable Loops by Model\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "for model in df_all['source_model'].unique():\n",
    "    print(f\"\\n{model.upper()}\")\n",
    "    print(\"-\" * len(model))\n",
    "    \n",
    "    top, bottom = get_extreme_loops(df_all, model)\n",
    "    \n",
    "    print(\"\\nTop 5 Presence Scores:\")\n",
    "    print(top.to_string())\n",
    "    \n",
    "    print(\"\\nBottom 5 Presence Scores:\")\n",
    "    print(bottom.to_string())\n",
    "    \n",
    "# Create presence score distribution plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=df_all, x='source_model', y='presence_score')\n",
    "plt.title(\"Presence Score Distribution by Model\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate z-scores for presence scores\n",
    "df_all['presence_zscore'] = (df_all['presence_score'] - df_all['presence_score'].mean()) / df_all['presence_score'].std()\n",
    "\n",
    "# Identify statistical outliers (|z-score| > 2)\n",
    "outliers = df_all[abs(df_all['presence_zscore']) > 2][\n",
    "    ['source_model', 'loop_id', 'title', 'presence_score', 'presence_zscore']\n",
    "].sort_values('presence_zscore', ascending=False)\n",
    "\n",
    "print(\"\\nâš ï¸ Statistical Outliers (|z-score| > 2)\")\n",
    "print(\"-\" * 40)\n",
    "print(outliers.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae33ca9",
   "metadata": {},
   "source": [
    "## 9. Cross-Model Analysis\n",
    "\n",
    "Let's examine how different models compare in terms of presence metrics and patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d991c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average metrics per tier for each model\n",
    "def get_tier_averages(df, tier='tier_I'):\n",
    "    \"\"\"Calculate average metrics for a specific tier across models\"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        model = row['source_model']\n",
    "        tier_data = row['metrics'][tier]\n",
    "        \n",
    "        # Skip resolution_type for tier_I\n",
    "        if tier == 'tier_I':\n",
    "            tier_data = {k: v for k, v in tier_data.items() if k != 'resolution_type'}\n",
    "            \n",
    "        tier_data['model'] = model\n",
    "        metrics.append(tier_data)\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    return metrics_df.groupby('model').mean()\n",
    "\n",
    "# Get averages for each tier\n",
    "tier1_avg = get_tier_averages(df_all, 'tier_I')\n",
    "tier2_avg = get_tier_averages(df_all, 'tier_II')\n",
    "tier3_avg = get_tier_averages(df_all, 'tier_III')\n",
    "\n",
    "# Create radar charts for each model\n",
    "def create_radar_chart(model_data, title):\n",
    "    categories = list(model_data.columns)\n",
    "    n_cats = len(categories)\n",
    "    \n",
    "    angles = [n / float(n_cats) * 2 * np.pi for n in range(n_cats)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    for model in model_data.index:\n",
    "        values = model_data.loc[model].values\n",
    "        values = np.concatenate((values, [values[0]]))\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=model)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    \n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Cross-Model Metric Comparison\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "print(\"\\nTier I Metrics:\")\n",
    "print(\"--------------\")\n",
    "print(tier1_avg.round(3))\n",
    "create_radar_chart(tier1_avg, \"Tier I Metrics by Model\")\n",
    "\n",
    "print(\"\\nTier II Metrics:\")\n",
    "print(\"---------------\")\n",
    "print(tier2_avg.round(3))\n",
    "create_radar_chart(tier2_avg, \"Tier II Metrics by Model\")\n",
    "\n",
    "print(\"\\nTier III Metrics:\")\n",
    "print(\"----------------\")\n",
    "print(tier3_avg.round(3))\n",
    "create_radar_chart(tier3_avg, \"Tier III Metrics by Model\")\n",
    "\n",
    "# Create correlation heatmap between models\n",
    "model_correlations = pd.DataFrame(index=df_all['source_model'].unique())\n",
    "\n",
    "for model in df_all['source_model'].unique():\n",
    "    model_scores = df_all[df_all['source_model'] == model]['presence_score']\n",
    "    for other_model in df_all['source_model'].unique():\n",
    "        other_scores = df_all[df_all['source_model'] == other_model]['presence_score']\n",
    "        correlation = model_scores.corr(other_scores)\n",
    "        model_correlations.loc[model, other_model] = correlation\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(model_correlations, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title(\"Cross-Model Presence Score Correlations\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a555b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "The enhanced analysis is complete. Here's what we've added:\n",
    "\n",
    "1. **Comprehensive Metrics Summary**\n",
    "   - Dataset statistics per model\n",
    "   - Metrics completeness analysis\n",
    "   - Distribution visualizations\n",
    "\n",
    "2. **Outlier Analysis**\n",
    "   - Top/bottom loops per model\n",
    "   - Statistical outliers (z-score based)\n",
    "   - Distribution plots\n",
    "\n",
    "3. **Cross-Model Analysis**\n",
    "   - Tier-wise metric comparisons\n",
    "   - Radar charts for metric patterns\n",
    "   - Model correlation heatmap\n",
    "\n",
    "### ðŸ“‹ Upcoming Notebooks\n",
    "\n",
    "The following companion notebooks are ready to be created:\n",
    "1. `Loop_Validation_Report.ipynb` - Schema integrity validation\n",
    "2. `Presence_Comparative_Analysis.ipynb` - Detailed model comparison\n",
    "3. `Ignition_Detection_Engine.ipynb` - First ignition analysis\n",
    "4. `Loop_Signature_Mapping.ipynb` - Pattern fingerprinting\n",
    "5. `Presence_Density_Heatmap.ipynb` - Signal strength visualization\n",
    "\n",
    "Choose which analysis path you'd like to explore next!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
